{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup and Initialization\n",
    "\n",
    "## 1.1 Importing Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System\n",
    "import wandb\n",
    "import os\n",
    "import json\n",
    "import gc\n",
    "\n",
    "# Environment\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "\n",
    "# LLM packages\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    "    set_seed,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "# Set Seed\n",
    "train_seed = 2002\n",
    "set_seed(train_seed)\n",
    "\n",
    "# 캐시 디렉토리 설정\n",
    "DATA_CACHE_DIR = \"/mnt/t7/.cache/huggingface/datasets\"\n",
    "MODEL_CACHE_DIR = \"/mnt/t7/.cache/huggingface/models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Logging into Hugging Face Hub and Weights & Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maeolian83\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"WANDB_PROJECT\"]=\"Graduate Project Llama-3.2-3B\"\n",
    "wandb.login()\n",
    "\n",
    "load_dotenv(\"/mnt/t7/dnn/llm_practicing/.env\")\n",
    "login(token= os.environ[\"HF_TOKEN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading and Preparing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"aeolian83/PTT_wit_Latex_1\"\n",
    "\n",
    "dataset_dict = load_dataset(dataset_name, cache_dir=DATA_CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['term', 'english', 'korean'],\n",
       "        num_rows: 1432\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_train = dataset_dict[\"train\"].shuffle(seed=42)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": shuffled_train,\n",
    "    # 다른 split도 있으면 추가 (예: \"validation\": dataset[\"validation\"])\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Hyperparameter Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configring Base Model Load \n",
    "model_id = \"meta-llama/Llama-3.2-3B\"\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "# Configuring Quantization\n",
    "load_in_4bit = True\n",
    "bnb_4bit_compute_dtype = torch.bfloat16\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "bnb_4bit_use_double_quant = True\n",
    "\n",
    "#Defining Training Arguments\n",
    "checkpoint_dir = \"/mnt/t7/dnn/paper_translator2/test/checkpoint/meta_Llama-3.2-3B_ft01\"\n",
    "output_dir = checkpoint_dir\n",
    "per_device_train_batch_size = 4\n",
    "gradient_accumulation_steps = 2\n",
    "optim = \"paged_adamw_32bit\"\n",
    "report_to=\"wandb\"\n",
    "save_strategy=\"epoch\"\n",
    "num_train_epochs = 6\n",
    "logging_steps = 20\n",
    "# eval_steps=100,\n",
    "learning_rate = 2e-4\n",
    "max_grad_norm = 0.3\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = \"cosine\"\n",
    "bf16 = True\n",
    "group_by_length = True\n",
    "\n",
    "# Configuring Lora\n",
    "lora_r = 64\n",
    "lora_alpha = 16\n",
    "lora_dropout=0.1\n",
    "target_modules='all-linear'\n",
    "bias=\"none\"\n",
    "task_type=\"CAUSAL_LM\"\n",
    "\n",
    "# Configuring tokenizer\n",
    "padding_side = \"right\"\n",
    "response_template = \"### Translated:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Loading the Base Model for QLoRA\n",
    "\n",
    "## 4.1 Loading the Model with QLoRA Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_use_double_quant=bnb_4bit_use_double_quant,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d78010cd7e4d96ab9d179216055083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config, device_map=device_map, cache_dir=MODEL_CACHE_DIR, low_cpu_mem_usage=True, trust_remote_code=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Loading the Tokenizer and Setting up Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=MODEL_CACHE_DIR)\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.pad_token = \"<|finetune_right_pad_id|>\"\n",
    "tokenizer.pad_token_id = 128004\n",
    "\n",
    "# tokenizer.padding_side = padding_side\n",
    "response_template = response_template\n",
    "\n",
    "data_collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training the Model\n",
    "## 5.1 Defining Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    save_strategy=save_strategy,\n",
    "    logging_steps=logging_steps,\n",
    "    # eval_steps=eval_steps,\n",
    "    report_to = report_to,\n",
    "    learning_rate=learning_rate,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    seed=train_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Defining PEFT Lora Configuration and Formatting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA Config\n",
    "peft_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    target_modules=target_modules,\n",
    "    bias=bias,\n",
    "    task_type=task_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting function\n",
    "def formatting_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example[\"english\"])):\n",
    "        text = f\"Translate input sentence to Korean \\n### Input: {example['english'][i]} \\n### Translated: {example['korean'][i]}\" + tokenizer.eos_token\n",
    "        output_texts.append(text)\n",
    "\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1) system_prompt: 교수님 역할만\n",
    "# system_prompt = \"You are a professor specializing in Physics, proficient in both Korean and English. Your task is to translate English physics content into Korean, adhering to specific guidelines.\"\n",
    "\n",
    "# # 2) user_prompt: 가이드라인과 실제 입력을 함께\n",
    "# user_prompt_template = \"\"\"<번역 지침>\n",
    "# 1. 매우 중요: 모든 전문 용어는 반드시 아래 형식으로 번역해야 합니다: 한글 용어(영문 용어). 예시: 적대적 훈련(adversarial training).\n",
    "# 2. 약어는 다음과 같은 형식을 사용하세요: 한글 전체 용어(영문 전체 용어, 약어). 예시: 계층적으로 조직된 경량 다중 탐지 시스템(hierarchically organized light-weight multiple detector system, HOLMES).\n",
    "# 3. 학술적인 문체를 유지하며, 기술적 정확성을 확보하세요.\n",
    "# 4. 원문의 의미를 정확하게 반영하면서도 자연스러운 한국어 문장으로 번역하세요.\n",
    "# 5. 번역 결과에 '*' 기호를 사용하지 마세요.\n",
    "# 6. 한국어 문장 내 괄호 안의 모든 영문자는 소문자로 표기하세요.\n",
    "# 7. 용어 및 괄호 번역 방식을 문장 전체에서 일관되게 유지하세요.\n",
    "# 8. 수식이나 수학적 표현을 번역할 때는 국내 물리학 논문의 표준 표기법을 따르세요.\n",
    "# </번역 지침>\n",
    "\n",
    "# ## 출력 예시\n",
    "# korean: 앙상블 학습(context of ensemble learning)에서 적응형 신경 프레임워크(adaptive neural frameworks)의 개발은 다양한 벤치마크 데이터셋(benchmark datasets)에서 광범위한 실험 결과로 입증된 바와 같이 심층 신경망(deep neural networks)의 성능을 크게 향상시킵니다. 이러한 적응형 신경 프레임워크(adaptive neural frameworks)를 활용함으로써 연구자들은 특징을 지능적으로 융합하여 더 차별화되고 효과적인 표현을 생성할 수 있으며, 이에 따라 모델의 일반화 능력을 향상시킬 수 있습니다. 결과적으로, 적응형 신경 프레임워크(adaptive neural frameworks)는 전통적인 특징 융합 기법(traditional feature fusion techniques)을 능가할 뿐만 아니라 이미지 분류(image classification), 객체 탐지(object detection), 자연어 처리(natural language processing, NLP), 그래프 기반 학습(graph-based learning) 작업을 포함한 여러 도메인에서 광범위한 적용 가능성을 보여줍니다.\n",
    "\n",
    "# ## 출력 형식\n",
    "# korean: 형식에 맞게 용어를 용어번역과 괄호 원문을 배칙하고, 잘 번역된 문장들\n",
    "\n",
    "# 참고: 반드시 한국어 번역문만 출력하세요. 원본 영어 문장은 포함하지 마세요.\n",
    "\n",
    "# Input: {english}\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Formatting function\n",
    "# def formatting_func(example):\n",
    "#     formatted_texts = []\n",
    "#     for eng, kor in zip(example[\"english\"], example[\"korean\"]):\n",
    "#         # SYSTEM 메시지\n",
    "#         convo = f\"[SYSTEM]\\n{system_prompt}\\n\\n\"\n",
    "#         # USER 메시지: guideline + 실제 입력\n",
    "#         user_prompt = user_prompt_template.format(english=eng)\n",
    "#         convo   += f\"[USER]\\n{user_prompt}\\n\"\n",
    "#         # ASSISTANT 메시지: 정답 + EOS\n",
    "#         convo   += f\"[ASSISTANT]\\nkorean: {kor}{tokenizer.eos_token}\"\n",
    "#         formatted_texts.append(convo)\n",
    "#     return formatted_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Setting up Trainer with SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94182/468261088.py:1: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=peft_config,\n",
    "    args=training_arguments,\n",
    "    train_dataset=dataset_dict[\"train\"],\n",
    "    formatting_func=formatting_func,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/t7/dnn/paper_translator2/test/wandb/run-20250612_171817-vs2jf6cl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aeolian83/Graduate%20Project%20Llama-3.2-3B/runs/vs2jf6cl' target=\"_blank\">/mnt/t7/dnn/paper_translator2/test/checkpoint/meta_Llama-3.2-3B_ft01</a></strong> to <a href='https://wandb.ai/aeolian83/Graduate%20Project%20Llama-3.2-3B' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aeolian83/Graduate%20Project%20Llama-3.2-3B' target=\"_blank\">https://wandb.ai/aeolian83/Graduate%20Project%20Llama-3.2-3B</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aeolian83/Graduate%20Project%20Llama-3.2-3B/runs/vs2jf6cl' target=\"_blank\">https://wandb.ai/aeolian83/Graduate%20Project%20Llama-3.2-3B/runs/vs2jf6cl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1074' max='1074' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1074/1074 25:47, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.916300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.634700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.558500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.459200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.455500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.428400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.401900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.375700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.370500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.307800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.265200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.300900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.259300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.295200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.277000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.270200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.263800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.241600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.153200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.195100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.162600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.188600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.163500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.180900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.170400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.151100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.115700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.089200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.124200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.096900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.117100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.099800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.107600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.103300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.088700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.075900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.055400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.079900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.058600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.074200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.062800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.068900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.064700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.054700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.055200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.042300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.056600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.044700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.056500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.046100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.056800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.052300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(os.path.join(training_arguments.output_dir, \"last_checkpoint\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Merge and Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del(trainer)\n",
    "# gc.collect()\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-3B\"\n",
    "device_map = {\"\": 0}\n",
    "checkpoint_dir = \"/mnt/t7/dnn/paper_translator2/test/checkpoint/meta_Llama-3.2-3B_ft01/last_checkpoint\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf6bf2a5c23f47d58c1b33423cc586e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id, \n",
    "            torch_dtype=torch.bfloat16, \n",
    "            load_in_8bit=False, \n",
    "            device_map=device_map, \n",
    "            trust_remote_code=True, \n",
    "            cache_dir=MODEL_CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = PeftModel.from_pretrained(model, checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e76747faf8d413790bd3f0bc220a2d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed0b553cb5c44019362294b0f4a2b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba49e9a0956848e49c9b4fc262a1a892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/aeolian83/meta_Llama-3.2-3B_translator01/commit/677f5e6b44128dd1508b812b74f6da6d931beb5c', commit_message='Upload LlamaForCausalLM', commit_description='', oid='677f5e6b44128dd1508b812b74f6da6d931beb5c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/aeolian83/meta_Llama-3.2-3B_translator01', endpoint='https://huggingface.co', repo_type='model', repo_id='aeolian83/meta_Llama-3.2-3B_translator01'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub('aeolian83/meta_Llama-3.2-3B_translator01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6ca7bb2b3a04116826e6ff1376db896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/aeolian83/meta_Llama-3.2-3B_translator01/commit/eb188dc351ad4bcc1d72abf02237a1df6ef1d38e', commit_message='Upload tokenizer', commit_description='', oid='eb188dc351ad4bcc1d72abf02237a1df6ef1d38e', pr_url=None, repo_url=RepoUrl('https://huggingface.co/aeolian83/meta_Llama-3.2-3B_translator01', endpoint='https://huggingface.co', repo_type='model', repo_id='aeolian83/meta_Llama-3.2-3B_translator01'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub('aeolian83/meta_Llama-3.2-3B_translator01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_for_p312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
